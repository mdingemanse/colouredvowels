---
title: "Coloured vowels: open data and code"
output:
  github_document: default
  html_document: 
    toc: true
    toc_depth: 3
    toc_float: true
    theme: united
---
```{r global_options, include=FALSE}
#run the outcommented render command below to generate both .html and .md output, the latter of which will be rendered nicely on GitHub:
#rmarkdown::render("BRM_colouredvowels_opendata.Rmd", output_format = "all",encoding="UTF-8")
knitr::opts_chunk$set(fig.path='figs/', echo=TRUE, warning=FALSE, message=FALSE)

```

```{r preliminaries, results='hide',include=F}

rm(list=ls())

# Packages and useful functions
list.of.packages <- c("tidyverse")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)>0) install.packages(new.packages)
lapply(list.of.packages, require, character.only=T)
rm(list.of.packages,new.packages)

`%notin%` <- function(x,y) !(x %in% y) 

load("BRM_colouredvowels_opendata.Rdata")

```

## Intro
This dataset and code accompanies the following paper on categorical perception and structure in vowel-colour mappings:

> Cuskley, C.<sup>1</sup>, Dingemanse, M.<sup>1</sup>, van Leeuwen, T. & Kirby, S. 2019. Cross-modal associations and synaesthesia: Categorical perception and structure in vowel-colour mappings in a large online sample. *Behaviour Research Methods*, doi: [10.3758/s13428-019-01203-7](https://doi.org/10.3758/s13428-019-01203-7).

<sup>1</sup> Joint first authors & corresponding authors: ccuskley@gmail.com, m.dingemanse@let.ru.nl.

## Data
The data was collected as part of a large-scale study into synaesthesia and cross-modal associations (*Groot Nationaal Onderzoek*, Van Leeuwen & Dingemanse 2016). Spoken vowel-colour association data was collected for **`r length(d.participants$anonid)` participants** using recordings of 16 vowel sounds selected to represent points spread through acoustic vowel space (Moos et al. 2014). Grapheme-colour association data was collected for a subset of 398 participants who took a full grapheme-colour association test, among them are around 100 confirmed synaesthetes.

The focus of this paper is on colour associations to spoken vowel sounds. Data comes in the following data frames (shared in .csv and .Rdata formats): `d.voweldata` for the raw data from the association task (with `d.stimuli` recording order of presentation) and `d.participants` for anonymised participant metadata and consistency and structure scores.

* [BRM_colouredvowels_voweldata.csv](/BRM_colouredvowels_voweldata.csv)
* [BRM_colouredvowels_stimuli.csv](/BRM_colouredvowels_stimuli.csv)
* [BRM_colouredvowels_participants.csv](/BRM_colouredvowels_participants.csv)

The raw data from the association task looks like this:

```{r data}
str(d.voweldata)

```
Here, `anonid` is an anonymised participant identifier; `setname` records the item randomisation a participant was exposed to (as specified in `d.stimuli`); `item` lists item as presented in the test; and `color` and `timing` record colour choice and RT per trial (each item is presented three times).

Participant metadata is in `d.participants` and looks like this:
```{r data-structure-profile}
str(d.participants)
```

The `anonid` allows linking across test results and metadata. Age and binarised gender were recorded using an optional pre-test questionnaire, which about 75% of participants filled out. Age (`age_bin`) is binned to ensure participant privacy; reported age range is 18-88 (median = 46, SD = 16).  Participants identifying as female are overrepresented, which is common in self-selecting online studies.

`ConsistencyScore` is an overall measure of consistency in colour choices, based on distances in CIELuv space. 34 out of 1164 participants received no consistency score as they chose "No colour" for more than half of the items. `SynStatus` is a synaesthesia classification based on the consistency score, as reported in the paper; we identified 365 synaesthetes. Finally, `r`, `StructureScore` (also called *z score* in the paper), and `p-value` are related to the novel Structure measure we introduce in the paper, which characterizes the degree to which participants' responses are structure isomorphically across modalities.

## Code
Part of our analysis is in Python. [BRM_colouredvowels_MantelCode.py](/BRM_colouredvowels_MantelCode.py) has the code for computing structure scores using the Mantel test.

Also, if you're interested in the code for the online cross-modal association test, have a look at [SenseTest](/SenseTest).

## Sneak peek

What does it look like when you ask people to associate colours to vowel sounds? Here are some samples from the data (more details in the paper). 

```{r examples, fig.width=10,fig.height=4}

library(ggrepel)
library(cowplot)

stims <- read_delim(file="BRM_colouredvowels_stimuli_properties.tsv",delim="\t") %>%
  plyr::rename(c("File" = "item","VowelCat" = "phoneme","Graphcat" = "grapheme"))

d.wide <- d.voweldata %>%
  mutate(item = as.numeric(item)) %>% # drop leading zero
  gather("trial","colour", starts_with("color")) %>%
  mutate(trial = sapply(trial,gsub,pattern="color",replacement="")) %>% # keep only trial number
  dplyr::select(anonid,item,trial,colour) %>%
  arrange(item) %>%
  left_join(stims) # add stim metadata for easy plotting

source(file="BRM_colouredvowels_functions.R")

plot.nonsyn <- vowelplot(pid="f999ef")
plot.syn <- vowelplot(pid="4e6aad")

plot_grid(plot.nonsyn, plot.syn, labels = c("A", "B"))


```

Colours chosen by a non-synaesthete (A) and a synaesthete (B) for 16 vowel sounds (each sound presented 3 times in total). While the synaesthete is more consistent in picking exactly the same colour for the same sound each time, both participants group sounds in terms of the vowel they belong to (for instance the three [i:]-like sounds, top left), and both structure their choices so that they pick lighter colours for [i:] than for [u:] (top right) and [a:] (bottom), revealing general principles underlying the associations.

